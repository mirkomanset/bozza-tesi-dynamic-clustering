\chapter{Preliminaries}\label{ch:preliminaries}

This chapter provides the foundational concepts required to understand the
methods and analyses presented in subsequent sections. It begins with an
overview of the supervised and unsupervised learning paradigms, highlighting
their core principles, objectives, and formal frameworks. Special focus is
placed on clustering, one of the primary tasks in unsupervised learning,
including its algorithms and evaluation metrics. The chapter then introduces
the stream learning paradigm, which addresses learning from evolving data
distributions. Finally, it discusses the challenges posed by data drift, along
with techniques for its detection, adaptation, and explanation.

\section{Supervised Learning}\label{sec:supervised_leaning}
Supervised Learning is a well-established paradigm in machine learning where an
algorithm is trained to map input data (features) to the corresponding output
variables (targets) of a labeled dataset. The aim is to learn a model that can
predict in the best way possible the outputs of new input data. Let $\mathbf{x}
    \in \mathcal{X}$ be the input vectors (features) and $ y \in \mathcal{Y}$ be
the associated output variables (targets). The training set consists of $N$
independent and identically distributed (i.i.d.) tuples $\{(\mathbf{x_i},
    y_i)\}^N_{i=1}$ drawn from an unknown joint probability distribution
$P(\mathbf{x},y)$.

The main goal of supervised learning is to approximate the underlying
input-output relation by means of a function $h \in \mathcal{H}$ with
$\mathcal{H}$ being the hypothesis space and such that $h: \mathcal{X}
    \rightarrow \mathcal{Y}$.

Formally given a learning model $\mathcal{L} = (\mathcal{H},\ell)$ which
includes a hypothesis space $\mathcal{H}$ and a loss function $\ell: \mathbb{R}
    \times \mathbb{R} \rightarrow \mathbb{R}$, the \emph{expected risk} of a
hypothesis $h \in \mathcal{H}$ is defined as
\begin{equation}
    R_P(h) = \mathbb{E}_P[\ell(h(\mathbf{x}),y)] = \int_{\mathcal{X}} \int_{\mathcal{Y}}
    \ell(h(\mathbf{x}),y)P(\mathbf{x},y)dyd\mathbf{x},
\end{equation}
where $\mathbb{E}_P$ denotes the expectation with respect to the joint distribution.
The objective is to find the hypothesis $h^* \in \mathcal{H}$ that minimizes
the expected risk:
\begin{equation}
    h^* = \underset{h \in \mathcal{H}}{\arg\min} R_P(h).
\end{equation}
However, since the distribution $P(\mathbf{x},y)$ is typically unknown, the
expected risk cannot be computed directly. Instead, an empirical version is
used, based on a finite set of training samples $D: = \{  (\mathbf{x}_i, y_i)  \}_{i=1}^N$.
The \emph{empirical risk} is computed as:
\begin{equation}
    \hat{R}_p(h, D) = {\frac{1}{N}} \sum^N_{i=1} \ell(h(\mathbf{x}_i),y_i),
\end{equation}
where samples $\{ (\mathbf{x}_i, y_i) \} \in D$.

This leads to the principle of \emph{empirical risk minimization}, where the
goal is to find the hypothesis $\hat{h}$ that minimizes the empirical risk
conditioned on the dataset $D$:
\begin{equation}
    \hat{h} = \underset{h \in \mathcal{H}}{\arg\min} \hat{R}_P(h, D).
\end{equation}

\section{Unsupervised Learning}\label{sec:unsupervised_learning}
Unsupervised learning refers to a branch of machine learning concerned with
uncovering underlying structure or patterns in a dataset without being provided
labeled outputs. In this setting, only input data $\mathbf{x} \in \mathcal{X}$
is available, typically drawn from an unknown probability distribution
$P(\mathbf{x})$, and no supervision signal (target variable) is provided.

The learning process aims to discover meaningful representations or groupings
that reveal properties of the data such as clusters, latent factors, or
manifolds. The precise goal of unsupervised learning depends on the specific
task, which may include clustering, dimensionality reduction, density
estimation, or representation learning.

Unlike supervised learning, where the risk is naturally defined with respect to
a prediction error on a known target, unsupervised learning often lacks a
unique objective function. Instead, each task introduces its own criteria:
\begin{itemize}
    \item In \emph{clustering}, algorithms aim to assign data points to groups such that
          intra-cluster similarity is maximized while inter-cluster similarity is
          minimized;
    \item In \emph{dimensionality reduction}, methods seek low-dimensional embeddings
          that preserve specific properties of the data, such as variance
          (PCA~\cite{pca}) or neighborhood structure (t-SNE~\cite{tsne},
          UMAP~\cite{umap});
    \item In \emph{density estimation}, the goal is to estimate the data-generating
          distribution $P(\mathbf{x})$ itself.
\end{itemize}

Formally, given a dataset $D:= (\mathbf{x}_i)_{i=1}^N$ many unsupervised
learning problems can be cast as a general optimization problem of the form:
\begin{equation}
    \hat{f} = \underset{f \in \mathcal{F}}{\arg\min} \mathcal{J}(f, D),
\end{equation}
where $\mathcal{F}$ is the space of candidate functions or models, and
$\mathcal{J}: \mathcal{F} \times \mathcal{D} \rightarrow \mathbb{R}$ is a task-specific loss function that quantifies the quality of
the learned structure (e.g., reconstruction error, likelihood, clustering
objective). Unlike in supervised learning, $\mathcal{J}$ does not directly
compare predictions to ground truth targets, but instead measures
inter-consistency or data fit.

\section{Clustering}\label{sec:clustering}

Clustering is a fundamental task in unsupervised learning that aims to
partition a dataset into groups, or \emph{clusters}, such that instances within
the same cluster are more similar to each other than to instances in different
clusters. It is a key technique for discovering underlying structure,
compressing data, detecting outliers, and analyzing patterns in
high-dimensional datasets. Formally, let

\begin{equation}
    D = \{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_N\},
\end{equation}
where each $\mathbf{x}_i \in \mathbb{R}^d$ is a data point sampled from an unknown distribution
$P(\mathbf{x})$. The clustering task involves assigning each point a label $c_i
    \in \{1, \ldots, K\}$, where $K \in \mathbb{N}$ is either predefined,
or selected via model selection techniques.
A clustering $C = \{C_1, C_2, \ldots, C_K\}$
must satisfy the following conditions:
\begin{equation}
    \begin{cases}
        C_k \subseteq D,          & \forall k \in \{1, \ldots, K\} \\
        C_i \cap C_j = \emptyset, & \forall i \neq j               \\
        \bigcup_{k=1}^{K} C_k = D.
    \end{cases}
\end{equation}

Each clustering algorithm defines an objective function $\mathcal{J}$, often
representing intra-cluster similarity or inter-cluster separation. The optimal
clustering is obtained by minimizing this objective over all admissible
partitions:
\begin{equation}
    \hat{C} = \underset{C \in \mathcal{C}}{\arg\min} \, \mathcal{J}(C; D),
\end{equation}
where $\mathcal{C}$ denotes the space of valid partitions of $D$.

\subsection{Clustering Algorithms}
A wide range of clustering algorithms has been developed, each based on
different assumptions and mechanisms. These include partition-based,
hierarchical, density-based, fuzzy, distribution-based, grid-based, and
model-based approaches~\cite{clustering_survey}. The methods differ in how they
define similarity, form clusters, and manage noise or outliers.

Among these, \textbf{K-means} and \textbf{Gaussian Mixture Models (GMM)} are
two widely used algorithms that effectively summarize data through compact
cluster representations. As examples of the partition-based and
distribution-based paradigms, respectively, both algorithms utilize summary
statistics, such as centroids, means, radii, and covariance matrices, to
characterize clusters. Such concise and informative descriptions are
particularly beneficial in applications like streaming clustering and clusters
tracking. Detailed descriptions of these algorithms are provided in the
following subsections.

\subsection*{K-means Clustering}
K-means~\cite{k_means} is a partition-based clustering algorithm that divides a
dataset into $k$ non-overlapping clusters by minimizing the within-cluster sum
of squared distances. Given a dataset $\mathcal{X} = \{\mathbf{x}_1,
    \mathbf{x}_2, \ldots, \mathbf{x}_N\}$ with $\mathbf{x}_i \in \mathbb{R}^d$,
K-means aims to find a set of $k$ centroids $\mu = \{\boldsymbol{\mu}_1,
    \boldsymbol{\mu}_2, \ldots, \boldsymbol{\mu}_k\}$ that minimize the following
objective function:

\begin{equation}
    J(\mathcal{X}, \mu) = \sum_{i=1}^{n} \sum_{j=1}^{k} q_{ij} || \mathbf{x}_i - \boldsymbol{\mu}_j ||_2^2,
\end{equation}

where:
\begin{itemize}
    \item $q_{ij} = 1$ if data point $\mathbf{x}_i$ is assigned to cluster $C_j$, and $0$ otherwise.
    \item $\boldsymbol{\mu}_j$ is the centroid of cluster $C_j$.
\end{itemize}

The algorithm proceeds iteratively via two steps:
\begin{enumerate}
    \item \textbf{Assignment step:} Assign each point to the nearest centroid:
          \begin{equation}
              q_{ij} =
              \begin{cases}
                  1 & \text{if } j = \arg \min_{l \in \{1, \ldots, k\}} ||\mathbf{x}_i - \boldsymbol{\mu}_l||_2^2 \\
                  0 & \text{otherwise}.
              \end{cases}
          \end{equation}
    \item \textbf{Update step:} Recompute the centroids as the mean of the assigned points:
          \begin{equation}
              \boldsymbol{\mu}_j = \frac{\sum_{i=1}^{n} q_{ij} \mathbf{x}_i}{\sum_{i=1}^{n} q_{ij}}.
          \end{equation}
\end{enumerate}

This process continues until convergence, typically when assignments no longer
change or the objective function stabilizes.

Once the final clusters are obtained, each cluster $ C_j $ can be summarized
using its centroid $ \boldsymbol{\mu}_j $ and a corresponding radius that
quantifies the cluster's spread. The radius $ r_j $ can be defined in several
ways; two common choices are:

the maximum Euclidean distance between the centroid and the points assigned to
the cluster:
\[
    r_j = \max_{\mathbf{x}_i \in C_j} ||\mathbf{x}_i - \boldsymbol{\mu}_j||_2,
\]

or the average Euclidean distance between the centroid and the points in the
cluster:
\[
    r_j = \frac{1}{|C_j|} \sum_{\mathbf{x}_i \in C_j} ||\mathbf{x}_i - \boldsymbol{\mu}_j||_2,
\]
where $ ||\mathbf{x}_i - \boldsymbol{\mu}_j||_2 $ denotes the Euclidean distance
between the point $ \mathbf{x}_i $ and the centroid $ \boldsymbol{\mu}_j $, and
$ |C_j| $ is the number of points in cluster $ C_j $.

K-means is effective when clusters are compact, spherical, and of similar size,
and when the number of clusters $k$ is known in advance. It performs well on
large, low-noise datasets where Euclidean distance is a meaningful similarity
metric. However, it may struggle with clusters of varying densities or
non-convex shapes.

\subsection*{Gaussian Mixture Models (GMM)}

GMM~\cite{gaussian_mixtures} is a distribution-based clustering method that
models data as a mixture of $k$ Gaussian distributions. Each component
(cluster) is represented by a \emph{multivariate Gaussian} with its own mean
and covariance which together define the shape, size, and orientation of the
cluster in the feature space.

The probability density function for a GMM is given by:

\begin{equation}
    p(\mathbf{x}_i) = \sum_{j=1}^{k} \pi_j \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_j, \mathbf{\mathbf{\Sigma}}_j),
\end{equation}

where:
\begin{itemize}
    \item $\pi_j$ is the mixing coefficient for cluster $C_j$, with $\sum_{j=1}^{k} \pi_j = 1$
    \item $\mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_j, \mathbf{\mathbf{\Sigma}_j})$ is the multivariate Gaussian density with mean $\boldsymbol{\mu}_j$ and covariance matrix $\mathbf{\Sigma}_j \in \mathbb{R}^{d \times d}$:
          \begin{equation}
              \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Sigma}) = \frac{1}{(2\pi)^{d/2} |\mathbf{\Sigma}|^{1/2}}
              \exp \left( -\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu})^\top \mathbf{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}) \right).
          \end{equation}
\end{itemize}

The parameters $\{\pi_j, \boldsymbol{\mu}_j, \mathbf{\Sigma}_j\}_{j=1}^k$ are
estimated using the \emph{Expectation-Maximization (EM)}
algorithm~\cite{em_algorithm}:

\begin{enumerate}
    \item \textbf{E-step:} Compute the posterior probabilities for each point:
          \begin{equation}
              \gamma_{ij} = \frac{\pi_j \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_j, \mathbf{\Sigma}_j)}
              {\sum_{l=1}^{k} \pi_l \mathcal{N}(\mathbf{x}_i \mid \boldsymbol{\mu}_l, \mathbf{\Sigma}_l)},
          \end{equation}

    \item \textbf{M-step:} Update parameters based on current posterior probabilities:
          \begin{align}
              \pi_j^{\text{new}}              & = \frac{1}{n} \sum_{i=1}^{n} \gamma_{ij},                                                                      \\
              \boldsymbol{\mu}_j^{\text{new}} & = \frac{\sum_{i=1}^{n} \gamma_{ij} \mathbf{x}_i}{\sum_{i=1}^{n} \gamma_{ij}},                                  \\
              \mathbf{\Sigma}_j^{\text{new}}  & = \frac{\sum_{i=1}^{n} \gamma_{ij}(\mathbf{x}_i - \boldsymbol{\mu}_j)(\mathbf{x}_i - \boldsymbol{\mu}_j)^\top}
              {\sum_{i=1}^{n} \gamma_{ij}}.
          \end{align}
\end{enumerate}

The EM algorithm iterates until the log-likelihood stabilizes. Unlike K-means,
GMM provides soft cluster assignments by computing the posterior probability of
each data point belonging to each cluster. The final hard cluster assignment
for a data point can be obtained by selecting the component with the highest
posterior probability:

\begin{equation}
    c_i = \arg\max_{j} \, \gamma_{ij},
\end{equation}

where $\gamma_{ij}$ denotes the posterior probability of the $j$-th component
for data point $\mathbf{x}_i$.

GMM is particularly effective when clusters have different shapes, sizes, and
orientations because it models each cluster with its own covariance matrix.
This flexibility allows GMM to capture elliptical or elongated clusters, unlike
K-means, which assumes spherical clusters of similar size due to its reliance
on Euclidean distance to the centroid.

Additionally, GMM's soft assignment approach provides a probabilistic
membership for each point, enabling a more detailed clustering.

\subsection{Clustering Evaluation Metrics}

Clustering evaluation metrics are broadly classified into \textbf{internal} and
\textbf{external} criteria. Internal metrics assess the quality of a clustering
using only the input data and the clustering result, without reference to any
external information. External metrics, on the other hand, require ground truth
labels to compare the clustering against known class memberships.

\paragraph{Silhouette Coefficient:} This metric measures how similar a data point is to its own cluster compared to
other clusters. For a data point $\mathbf{x}_i$, the silhouette score is
defined as:
\begin{equation}
    s(\mathbf{x}_i) = \frac{b(\mathbf{x}_i) - a(\mathbf{x}_i)}{\max\{a(\mathbf{x}_i), b(\mathbf{x}_i)\}},
\end{equation}
where $a(\mathbf{x}_i)$ is the average distance from $\mathbf{x}_i$ to all other points in its
cluster, and $b(\mathbf{x}_i)$ is the smallest average distance from $\mathbf{x}_i$ to points in a
different cluster. The coefficient ranges from $-1$ to $1$, where values close
to $1$ indicate well-clustered points.

\paragraph{Davies-Bouldin Index (DBI):} This metric evaluates the average similarity between each cluster and its most
similar cluster, defined as the sum of the within-cluster distances divided by
the between-cluster centroid distances. Formally:
\begin{equation}
    \text{DBI} = \frac{1}{k} \sum_{i=1}^{k} \max_{j \ne i} \left( \frac{\mathbf{\delta}_i + \mathbf{\delta}_j}{d_{ij}} \right),
\end{equation}
where $\mathbf{\delta}_i$ is the average distance of points in cluster $C_i$ to its
centroid, and $d_{ij}$ is the distance between the centroids of clusters $C_i$
and $C_j$. Lower values of DBI indicate better clustering.

\paragraph{Adjusted Rand Index (ARI):} The ARI compares the similarity between the predicted clusters and the ground
truth labels, correcting for chance groupings.

The \emph{Rand Index (RI)} measures the proportion of point pairs whose
clustering assignments are consistent between the predicted and true labels.
Formally, it is defined as:
\begin{equation}
    \text{RI} = \frac{TP + TN}{\binom{N}{2}},
\end{equation}
where:
\begin{itemize}
    \item $TP$ is the number of pairs of points that are in the same cluster in both the predicted and true clusterings,
    \item $TN$ is the number of pairs of points that are in different clusters in both the predicted and true clusterings,
    \item $\binom{N}{2}$ is the total number of unique pairs among $N$ data points.
\end{itemize}

The Adjusted Rand Index is then computed as:
\begin{equation}
    \text{ARI} = \frac{\text{RI} - \mathbb{E}[\text{RI}]}{\max(\text{RI}) - \mathbb{E}[\text{RI}]},
\end{equation}
where $\mathbb{E}[\text{RI}]$ is the expected value of the Rand Index under random assignments. ARI values range from $-1$ to $1$, with $1$ indicating perfect agreement.

\paragraph{Normalized Mutual Information (NMI):} NMI quantifies the agreement between the clustering assignments and the ground
truth labels by measuring how much information is shared between them. Given a
clustering result $ C = \{C_1, \dots, C_k\} $ and ground truth classes $ C' =
    \{C_1', \dots, C_l'\} $, the Normalized Mutual Information is defined as:
\begin{equation}
    \text{NMI}(C, C') = \frac{2 \cdot I(C, C')}{H(C) + H(C')},
\end{equation}
where $ I(C, C') $ is the mutual information between $ C $ and $ C' $, and $ H(C) $, $ H(C') $ are the entropies of the cluster and class distributions, respectively.

The mutual information $ I(C, C') $ is computed as:
\begin{equation}
    I(C, C') = \sum_{i=1}^{k} \sum_{j=1}^{l} P(C_i \cap C_j') \log \left( \frac{P(C_i \cap C_j')}{P(C_i) P(C_j')} \right),
\end{equation}
where $ P(C_i) $ is the probability that a randomly selected data point falls into cluster $ C_i $, and $ P(C_j') $ is the probability it belongs to ground truth class $ C_j' $.

The entropy of the cluster assignment $ C $ is defined as:
\begin{equation}
    H(C) = -\sum_{i=1}^{k} P(C_i) \log P(C_i),
\end{equation}
and similarly for $ H(C') $.

NMI ranges from $ 0 $ (no mutual information) to $ 1 $ (perfect correlation),
and is particularly robust when the number of predicted clusters differs from
the number of true classes.

Each metric provides a different perspective on clustering quality. Internal
metrics, such as the Silhouette score and the Davies-Bouldin Index (DBI),
evaluate the cohesion and separation of clusters based solely on the input
data, making them essential when no ground truth labels are available. In
contrast, external metrics like the Adjusted Rand Index (ARI) and Normalized
Mutual Information (NMI) compare the predicted clustering with known labels,
offering direct validation when labeled data is present.

\section{Stream Learning}\label{sec:stream_learning}
In many real-world applications, data is not available as a static, finite
dataset but rather arrives continuously over time in the form of a stream.
Stream Learning, also known as Online Learning or Incremental Learning, is a
machine learning paradigm designed to handle such continuously evolving data
environments. Unlike traditional batch learning methods, stream learning
algorithms are capable of processing data points individually or in small
batches, often in a single pass, and updating their models incrementally.

Formally, let $\{\mathbf{x}_1, \mathbf{x}_2,\ldots \} $ be a potentially
infinite sequence of data points drawn from a stream. The stream learning
algorithm maintains a model $\mathcal{M}_t$ at each time step $t$ which is
updated upon receiving a new data point $\mathbf{x}_t$. The update is performed
using a function $\mathcal{F}$ such that:
\begin{equation}
    \mathcal{M}_{t+1} = \mathcal{F}(\mathcal{M}_t, \mathbf{x}_t)
\end{equation}
This formulation enables the model to adapt continuously over time while
operating within bounded memory and computational constraints, as required
in streaming scenarios where incoming data can be potentially infinite.

This paradigm poses unique challenges, particularly in maintaining stability
while adapting to new information and detecting changes in the underlying data
distribution.

\section{Explainability in Machine Learning}\label{sec:explainability}

Explainability in Machine Learning refers to the capacity to understand and
interpret the internal mechanics or predictions of a model. As machine learning
systems are increasingly deployed in high-stakes domains such as healthcare,
finance, and autonomous systems, understanding how and why decisions are made
is essential for building trust, ensuring accountability, and complying with
ethical or legal standards~\cite{importance_of_explainabilty}.

Some models are inherently interpretable such as decision trees, linear
regression, and logistic regression where the influence of each feature is
explicitly encoded in the model's structure or coefficients. For example, in
linear models, the prediction $\hat{y}$ for an instance $\mathbf{x} \in
    \mathbb{R}^d$ is given by:
\begin{equation}
    \hat{y} = \mathbf{w}^\top \mathbf{x} + \mathbf{b},
\end{equation}
where $\mathbf{w} \in \mathbb{R}^d$ and each coefficient $w_i$ directly indicates the contribution of feature
$x_i$ to the prediction.

However, many high-performing models such as deep neural networks (DNNs),
support vector machines (SVMs), and ensemble methods like Random Forests and
Gradient Boosting Machines are considered \emph{black boxes} due to their
complexity and lack of direct interpretability.

To address this, a variety of \emph{post-hoc} interpretability techniques have
been developed. These can be broadly categorized into global (model-level) and
local (prediction-level) explanations.

One widely used approach is \textbf{feature importance}, which quantifies the
contribution of each input feature to the model's overall performance:

\begin{itemize}
    \item In tree-based ensamble models~\cite{feature_importance}, feature importance is
          often computed by summing the decrease in impurity (e.g., Gini index or
          entropy) resulting from splits on that feature across all trees: \begin{equation} \text{Importance}(f_i) =
              \sum_{t \in T} \sum_{s \in \text{Splits}(t, f_i)} \Delta \text{Impurity}(s),
          \end{equation}
          where $T$ is the set of trees in the ensemble, and $s$ are the split points
          involving feature $f_i$ with $i \in \{ 1, \ldots, d\}$.

    \item In \textbf{permutation importance}~\cite{permutation_importance}, the values of
          feature $f_i$ are randomly permuted to break the association with the target,
          and the drop in performance (e.g., accuracy or $R^2$) is recorded:
          \begin{equation} \text{Importance}_{\text{perm}}(f_i) =
              \mathcal{E}(\mathcal{D}) - \mathcal{E}(\mathcal{D}_{\pi_i}),
          \end{equation}
          where $\mathcal{E}(\cdot)$ is the model evaluation metric, $\pi_i$ represents a
          permutation for feature $f_i$ and
          $\mathcal{D}_{\pi_i}$ denotes the dataset with feature $f_i$ permuted.
\end{itemize}

\textbf{Local interpretability} methods explain individual predictions by approximating
the decision boundary around a specific input:

\begin{itemize}
    \item \textbf{LIME} (Local Interpretable Model-agnostic Explanations)~\cite{lime} fits a
          simple surrogate model (usually linear) around the neighborhood of the instance $\mathbf{x}$. It does so by generating a set of perturbed samples $\mathcal{Z}$ around $\mathbf{x}$ and minimizing a locality-weighted loss:
          \begin{equation}
              \mathcal{J}(f, g, \pi_x) = \sum_{\mathbf{z} \in \mathcal{Z}} \pi_x(\mathbf{z}) (f(\mathbf{z}) - g(\mathbf{z}))^2 + \Omega(g),
          \end{equation}
          where $f$ is the original model, $g$ is the interpretable surrogate, $\pi_x(\mathbf{z})$
          is a locality-aware kernel that assigns higher weight to points closer to $\mathbf{x}$, and $\Omega(g)$ penalizes the complexity of the surrogate model.

    \item \textbf{SHAP} (SHapley Additive exPlanations)~\cite{shap} assigns each feature an
          additive contribution to the prediction based on Shapley values from cooperative game theory:
          \begin{equation}
              f(\mathbf{x}) = \phi_0 + \sum_{i=1}^{d} \phi_i,
          \end{equation}
          where $\phi_0$ is the expected model output and $\phi_i$ quantifies the
          contribution of feature $i$ to the prediction. SHAP values are uniquely
          characterized by properties such as \emph{local accuracy},
          \emph{missingness}, and \emph{consistency}.
\end{itemize}

These tools help practitioners better understand, trust, and debug machine
learning models. As machine learning continues to permeate real-world
applications, explainability will remain a cornerstone of responsible AI
development.

\section{Data Drift}\label{sec:data_drift}
Data drift refers to the phenomenon where the statistical properties of data
change over time. This can lead to reduced model performance if the model is
not adapted to the evolving data distribution. In real-world applications, the
phenomenon of drift is often encountered due to changing environments, user
behavior, or external factors. Addressing data drift is crucial for maintaining
the reliability and accuracy of machine learning models over time. Drift can
occur in various forms, such as changes in the input data (input drift) or in
the underlying relationships between the data and the target (concept drift).
Detecting and adapting to data drift is an essential part of maintaining
long-term model performance.

\subsection*{Drift Definitions}\label{subsec:drift_definitions}
Following the definitions provided by Gama et
al.~\cite{drift_adaptation_survey}, this section outlines formal definitions of
\textbf{concept drift} and its key variants.

Concept drift is said to occur between two time points $t$ and $\tau$ if:
\begin{equation}
    \exists X : p_{t}(X, y) \neq p_{\tau}(X, y),
\end{equation}
where $p_t(X, y)$ represents the joint distribution of the input variables $X$
and the target variable $y$ at time $t$.

This change in the joint distribution can be decomposed into the following
components:
\begin{itemize}
    \item a shift in the marginal distribution of the inputs: $p(X)$,
    \item a change in the class-conditional distribution of the inputs: $p(y \mid X)$.
\end{itemize}

From the perspective of predictive modeling, two principal types of concept
drift are commonly distinguished:

\begin{enumerate}
    \item \textbf{Real Concept Drift}: Defined as a change in the posterior
          distribution over time:
          \begin{equation}
              p_{t}(y \mid X) \neq p_{\tau}(y \mid X)
          \end{equation}
          This type of drift affects the underlying decision function.

    \item \textbf{Virtual Concept Drift}: Occurs when the marginal distribution
          of the input features changes while the posterior distribution remains stable:
          \begin{equation}
              p_{t}(X) \neq p_{\tau}(X) \quad \text{and} \quad p_{t}(y \mid X) =
              p_{\tau}(y \mid X)
          \end{equation}
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{my_images/drift_types.png}
    \caption{Types of concept drift. Source:~\cite{drift_adaptation_survey}.}\label{fig:concept_drift_types}
\end{figure}

Although real concept drift is formally defined by changes in $ p(y \mid X) $,
in practice, such changes may be accompanied by observable shifts in the
marginal input distribution $ p(X) $, particularly when certain input regions
become more prevalent or undergo structural changes. However, changes in $ p(X)
$ alone do not guarantee concept drift, as the conditional relationship $ p(y
    \mid X) $ may remain unchanged. Nevertheless, analyzing how $ p(X) $ evolves
over time can offer indirect clues about the presence or potential causes of
concept drift, especially in situations where true labels are unavailable.

In addition to classifying concept drift based on its effect on the data
distribution (i.e., real vs.\ virtual drift), it is also important to consider
the \textit{temporal dynamics} through which drift unfolds. Based on the
taxonomy introduced by Gama et al.~\cite{drift_adaptation_survey}, the temporal
characteristics of concept drift can be categorized as follows:

\begin{itemize}
    \item \textbf{Abrupt drift}: A sudden and immediate change in the joint
          distribution, where the new concept replaces the old one almost
          instantaneously.

    \item \textbf{Incremental drift}: A gradual transition in which the old
          concept is progressively replaced by the new one, potentially passing
          through a continuum of intermediate states.

    \item \textbf{Gradual drift}: A process where both old and new concepts
          coexist for a period, with the probability of observing the new concept
          increasing over time.

    \item \textbf{Recurring drift}: A reappearance of previously
          observed concepts, typically in a periodic or seasonal manner, indicating
          temporal regularity in the drift process.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{my_images/drift_temporal_evolution.png}
    \caption{Temporal characterization of concept drift. Source:~\cite{drift_adaptation_survey}.}\label{fig:concept_drift_characterization}
\end{figure}

\subsection*{Drift Detection}\label{subsec:drift_detection}
Concept drift detection identifies changes in the statistical properties of
data streams that affect model performance. A typical framework consists of
four main stages~\cite{learning_under_concept_drift}:

\begin{enumerate}
    \item \textbf{Data Retrieval}: Organizes incoming data into chunks
          suitable for analysis.
    \item \textbf{Data Modeling (Optional)}: Transforms or reduces the data
          for efficiency or interpretability.
    \item \textbf{Test Statistic Computation}: Quantifies differences between
          past and current data or model performance.
    \item \textbf{Hypothesis Testing}: Determines whether detected differences
          are statistically significant.
\end{enumerate}

These stages are common across most detection algorithms and support flexible
implementation across streaming scenarios.

Drift detection algorithms can be broadly categorized into three groups based
on the nature of their test statistics.

\textbf{Error-Rate-Based Methods} monitor the performance of a predictive model.
A significant increase in error rate suggests concept drift.
Notable algorithms include:

\begin{itemize}
    \item \emph{DDM} (Drift Detection Method)~\cite{ddm}: Detects concept drift
          by monitoring the classification error rate and triggering a warning or drift s
          ignal when it statistically deviates from its historical minimum.

    \item \emph{EDDM}~\cite{eddm}: Extends DDM by analyzing the distance between
          consecutive errors, improving sensitivity to gradual and progressive drifts.

    \item \emph{ADWIN}~\cite{adwin}: Uses a variable-length sliding window and
          detects drift by statistically comparing averages in two subwindows, allowing
          automatic adaptation to both abrupt and gradual changes.
\end{itemize}

\textbf{Distribution-Based Methods} compare statistical properties of data distributions directly,
often in an unsupervised setting.

\begin{itemize}
    \item \emph{LSDD}~\cite{lsdd}: Uses a kernel-based method to measure the difference
          between distributions.
    \item \emph{ITA}~\cite{ita} (Information-Theoretic Approach): Relies on entropy and divergence
          measures to detect distributional changes.
    \item \emph{SyncStream}~\cite{syncstream}: Captures synchronized structural changes in multidimensional streams.
\end{itemize}

\textbf{Ensemble and Multi-Test Methods} use multiple statistical tests or detectors in parallel or
hierarchically to improve robustness and adaptivity.

\begin{itemize}
    \item \emph{JIT} (Just-in-Time Drift Detection)~\cite{jit}: Tests various feature combinations
          for localized drift detection.
    \item \emph{HHT} (Hierarchical Hypothesis Testing)~\cite{hht}: Combines quick detection with
          secondary validation to reduce false alarms.
    \item \emph{e-Detector}~\cite{e_detector}: Leverages an ensemble of heterogeneous detectors to
          balance early detection and accuracy.
\end{itemize}

Each category addresses different aspects of the drift detection problem.
Error-rate-based methods are tightly coupled with the model but rely on labeled
data. Distribution-based methods are model-agnostic and can work without
labels, while ensemble and hierarchical approaches aim to improve detection
reliability under varying conditions.

\subsection*{Drift Adaptation}\label{subsec:drift_adaption}

Concept drift adaptation refers to the broad set of techniques designed to
maintain predictive performance in the presence of changing data distributions.
This field is commonly referred to as \emph{Adaptive Learning}.

Gama et al.~\cite{drift_adaptation_survey} classify adaptation strategies into
two primary categories:

\begin{enumerate}
    \item \textbf{Blind methods}: These methods adapt the model continuously or
          periodically, without relying on explicit drift detection mechanisms.
          They are typically simpler and more responsive but may incur unnecessary
          updates when no drift has occurred.
    \item \textbf{Informed methods}: These approaches rely on the output of drift
          detectors to guide the adaptation process. By updating the model only when drift
          is detected, they aim to achieve more efficient and targeted adaptation.
\end{enumerate}

Another perspective proposed by Lu et al.~\cite{learning_under_concept_drift}
focuses on how the model is updated:

\begin{itemize}
    \item \textbf{Simple retraining}: Upon detection of concept drift, the
          current model is discarded and a new one is trained using the most recent
          data. A sliding or adaptive window is often used to select relevant training
          samples.
    \item \textbf{Model adjusting}: Instead of retraining the entire model,
          only affected parts are incrementally updated. This approach is particularly
          effective for handling localized or gradual drift, and is generally more
          efficient.
    \item \textbf{Ensemble retraining}: Rather than relying on a single model,
          ensemble methods maintain a pool of base classifiers. Adaptation is achieved
          by updating, weighting, replacing, or adding classifiers in response to drift,
          making them especially well-suited for handling recurring or complex drifts.
\end{itemize}

The effectiveness of drift adaptation methods depends on the characteristics of
the data stream, such as the type and frequency of drift. A well-designed
adaptive system often combines both detection and adaptation mechanisms to
ensure timely and efficient responses to changes in data distribution.

\subsection*{Drift Explainability}\label{subsec:drift_explainability}

Drift explainability focuses on understanding the causes and nature of changes
in data distributions that affect machine learning models. While detecting
drift is important, it is equally critical to explain why and how drift occurs.
This understanding provides valuable insights into the data evolution and
informs decisions about retraining and maintenance.

There are several techniques for explaining drift, each aiming to reveal how
data shifts influence model behavior. One common approach involves identifying
which features have changed significantly over time and assessing how these
changes impact the model's predictions. For example, \textbf{feature importance
    analysis}~\cite{feture_importance_for_drift_explainabilty} can be used to
compare a model $ f_{\text{ref}} $ trained on reference data $ D_P =
    \{(\mathbf{x}_i, y_i)\}_{i=1}^n \sim P $, where $ P $ is the reference data
distribution and $ n $ is the number of reference samples, with a model $
    f_{\text{prod}} $ trained on production data $ D_Q = \{(\mathbf{x}_j,
    y_j)\}_{j=1}^m \sim Q $, where $ Q $ is the production data distribution and $
    m $ is the number of production samples. Let $ \phi_i^{\text{ref}} $ and $
    \phi_i^{\text{prod}} $ denote the importance of feature $ i $ in the respective
models. A large shift
\begin{equation}
    \Delta \phi_i = \left| \phi_i^{\text{ref}} - \phi_i^{\text{prod}} \right|
\end{equation}
may indicate that feature $ i $ has changed its predictive relationship with the target.

Another powerful approach to drift explainability formulates the detection of
distributional shift as a binary classification problem between the reference
dataset $D_P$ and the production dataset $D_Q$. This method, known as a
\textbf{Classifier Two-Sample Test
    (C2ST)}~\cite{revisiting_two_sample_classifier}, labels samples from $P$ with
class 0 and samples from $Q$ with class 1, and trains a binary classifier $f:
    \mathcal{X} \rightarrow \{0, 1\}$ to distinguish between them.

If the null hypothesis $H_0: P = Q$ holds, then the classification accuracy of
$f$ on a held-out validation set $D_{\text{test}}$ should remain close to
chance level, i.e., $\text{Acc}(f) \approx 0.5$. The test statistic is given
by:
\begin{equation}
    \text{C2ST}(f) = \frac{1}{|D_{\text{test}}|} \sum_{(\mathbf{x}, y) \in D_{\text{test}}} \mathds{1} \{f(\mathbf{x}) = y\}.
\end{equation}
A significant deviation from $0.5$ suggests that $P \neq Q$, indicating distributional drift.

Beyond detection, C2STs offer an interpretable means to explain drift by
examining the feature importances of the trained classifier. This technique
highlights which features $x_i$ contribute most to $f(\mathbf{x})$, and hence
to the difference between $P$ and $Q$.

Understanding the factors behind drift is essential for effective model
management. Without explainability, decisions about retraining may be based on
guesswork, potentially leading to unnecessary updates or degraded performance.
Drift explainability ensures that model updates are both necessary and
well-targeted, guided by principled statistical and model-based insights.