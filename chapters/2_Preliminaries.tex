\chapter{Preliminaries}\label{ch:preliminaries}

\section{Supervised Learning}\label{sec:supervised_leaning}
Supervised Learning is a well-established paradigm in machine learning where an
algorithm is trained to map input data (features) to the corresponding output
variables (targets) of a labeled dataset.The aim is to learn a model that can
predict in the best way possible the outputs of new input data. Let $\mathbf{x}
    \in \mathcal{X}$ be the input vectors (features) and $ y \in \mathcal{Y}$ be
the associated output variables (targets). The training set consists of $N$
independent and identically distributed (i.i.d.) tuples $\{(\mathbf{x_i},
    y_i)\}^N_{i=1}$ drawn from an unknown joint probability distribution
$P(\mathbf{x},y)$.

The main goal of supervised learning is to approximate the underlying
input-output relation by inducing a function $h(\mathbf{x})$,that generalizes
and can make predictions for new unseen data.

Formally given a learning model $\mathcal{L} = (\mathcal{H},\ell)$ which
includes a hypothesis space $\mathcal{H}$ and a loss function $\ell: \mathbb{R}
    \times \mathbb{R} \rightarrow \mathbb{R}$, the \textit{expected risk} of a
hypothesis $h \in H$ is defined as
\[ 
    R_P(h) = \mathbb{E}_P[\ell(h(\mathbf{x}),y)] = \int_{\mathcal{X}} \int_{\mathcal{Y}}
    \ell(h(\mathbf{x}),y)P(\mathbf{x},y)dyd\mathbf{x}
\]
The objective is to find the hypothesis $h^* \in \mathcal{H}$ that minimizes
the expected risk:
\[ 
    h^* = \underset{h \in \mathcal{H}}{\arg\min} R_P(h)
\]
However, since the distribution $P(\mathbf{x},y)$ is typically unknown, the
expected risk cannot be computed directly. Instead, an empirical version is
used, based on a finite set of training samples. The \textit{empirical risk} is
computed as:
\[
    \hat{R}_p(h) = {\frac{1}{N}} \sum^N_{i=1} \ell(h(\mathbf{x_i}),y_i)
\]
This leads to the principle of \textit{empirical risk minimization}, where the
goal is to find the hypothesis $\hat{h}$ that minimizes the empirical risk:
\[
    \hat{h} = \underset{h \in \mathcal{H}}{\arg\min} \hat{R}_P(h)
\]

\section{Unsupervised Learning}\label{sec:unsupervised_learning}
Unsupervised Learning refers to a class of machine learning methods where the
aim is to uncover underlying structure or patterns in a dataset without being
provided labeled outputs. In this setting, only input data $\mathbf{x} \in
    \mathcal{X}$ is available, typically drawn from an unknown probability
distribution $P(\mathbf{x})$, and no supervision signal (target variable) is
provided.

The learning process aims to discover meaningful representations or groupings
that reveal properties of the data such as clusters, latent factors, or
manifolds. The precise goal of unsupervised learning depends on the specific
task, which may include clustering, dimensionality reduction, density
estimation, or representation learning.

Unlike supervised learning, where the risk is naturally defined with respect to
a prediction error on a known target, unsupervised learning often lacks a
unique objective function. Instead, each task introduces its own criteria:
\begin{itemize}
    \item in clustering, algorithms aim to assign data points to groups such that
          intra-cluster similarity is maximized while inter-cluster similarity is
          minimized;
    \item in dimensionality reduction, methods seek low-dimensional embeddings that
          preserve specific properties of the data, such as variance (PCA) or
          neighborhood structure (t-SNE, UMAP);
    \item in density estimation, the goal is to estimate the data-generating distribution
          $P(\mathbf{x})$ itself.
\end{itemize}
Formally, many unsupervised learning problems can be cast as optimization tasks of the form:
\[
    \hat{h} = \underset{f \in \mathcal{F}}{\arg\min} \mathcal{L}(f; \mathbf{x_1,\ldots,x_N}),
\]
where $\mathcal{F}$ is the space of candidate functions or models, and
$\mathcal{L}$ is a task-specific loss function that quantifies the quality of
the learned structure (e.g.\ reconstruction error, likelihood, clustering
objective). Unlike in supervised learning, $\mathcal{L}$ does not directly
compare predictions to ground truth targets, but instead measures
inter-consistency or data fit.

\section{Clustering}\label{sec:clustering}
Clustering is a fundamental task in unsupervised learning that aims to
partition a dataset into groups, or \textit{clusters}, such that data points
within the same cluster are more similar to each other than to those in
different clusters. It serves as a tool for structure discovery, summarization,
and drift analysis in high-dimensional data.

Given a dataset $\mathcal{D} = \{\mathbf{x_1, x_2, \ldots, x_N} \} \subset
    \mathbb{R}^d$, where each point $\mathbf{x_i}$ is drawn from an unknown
distribution $P(\mathbf{x})$, the goal of clustering is to assign each data
point to a cluster label $c_i \in \{1,\ldots, K\}$, where $K$ is either
predefined or determined through model selection criteria. Clustering can be
seen as an optimization problem over a set of candidate partitions
$\mathcal{C}$, where each partition $\mathcal{C} = \{ C_1, C_2, \ldots, C_K\}$
satisfies: 
\[
    \begin{cases}
        C_k \subseteq \mathcal{D} \\
        C_i \cap C_j = \emptyset  \\
        \bigcup^K_{k=1} C_k = \mathcal{D}
    \end{cases}
\]

Each clustering algorithm defines an objective function $\mathcal{L}$ to be
minimized, capturing the notion of intra-cluster similarity or inter-cluster
separation: 
\[ 
    \hat{\mathcal{C}} = \underset{\mathcal{C} \in
        \mathbf{S}}{\arg\min \mathcal{L}(\mathcal{C;D})} 
\] 
where $\mathbf{S}$ is the space of all admissible partitions.

Several clustering paradigms exist, including partition-based, hierarchical,
density-based, and model-based approaches.

\section{Stream Learning}\label{sec:stream_learning}
In many real-world applications, data is not available as a static, finite dataset but rather arrives continuously over time in the form of a stream. Stream Learning, also known as Online Learning or Incremental Learning, is a machine learning paradigm designed to handle such continuously evolving data environments. Unlike traditional batch learning methods, stream learning algorithms are capable of processing data points individually or in small batches, often in a single pass, and updating their models incrementally.

Formally, let $\{\mathbf{x_1, x_2,\ldots} \} $ be a potentially infinite sequence
of data points drawn from a stream. The stream learning algorithm maintains a
model $\mathcal{M}_t$ at each time step $t$ which is updated upon receiving a
new data point $\mathbf{x}_t$. The update is performed using a function $U$
such that: 
\[ 
    \mathcal{M}_{t+1} = U(\mathcal{M}_t, \mathbf{x_t}) 
\] 
This formulation allows the model to evolve over time while using bounded
memory and computation, a critical requirement in streaming scenarios.

Key characteristics of stream learning include:
\begin{itemize}
    \item single-pass constraints: each data point is typically processed once due to
          time and memory limitations;
    \item bounded resources: algorithms must operate under strict memory and
          computational constraints;
    \item adaptivity: the learning process must be robust to changes in the data
          distribution over time.
\end{itemize}

This paradigm poses unique challenges, particularly in maintaining stability
while adapting to new information and detecting changes in the underlying data
distribution.

\section{Explainability in Machine Learning}\label{sec:explainability}
Explainability in Machine Learning refers to the ability to understand and interpret 
the decisions made by machine learning models. As Machine Learning systems are deployed 
in critical areas such as healthcare, finance, and autonomous driving, the need for 
transparency and interpretability becomes crucial. Understanding why a model produces 
a particular output allows users to gain insights into the reasoning behind the model's 
predictions, leading to more informed decision-making.

The importance of explainability extends to several practical areas. It aids in
model validation and debugging, enabling developers to identify potential
issues such as biases, errors, or inefficiencies in the model. Furthermore,
explainability helps in improving model performance by revealing how different
features influence predictions. In settings where models are constantly being
updated or retrained with new data, interpretability ensures that the model
continues to function as expected and allows for more effective adjustments
when necessary.

While machine learning models, particularly deep learning and ensemble methods,
often achieve high accuracy, they tend to operate as `black boxes', making
their decision-making process difficult to understand. As a result, various
techniques have been proposed to improve the interpretability of these models.
Methods such as LIME (Local Interpretable Model-agnostic Explanations) and SHAP
(Shapley Additive Explanations) provide model-agnostic approaches to explain
individual predictions, while efforts are also being made to develop inherently
interpretable model architectures.

\section{Data Drift}\label{sec:data_drift}
Data drift refers to the phenomenon where the statistical properties of data
change over time. This can lead to reduced model performance if the model is
not adapted to the evolving data distribution. In real-world applications, the
concept of drift is often encountered due to changing environments, user
behavior, or external factors. Addressing data drift is crucial for maintaining
the reliability and accuracy of machine learning models over time. Drift can
occur in various forms, such as changes in the input data (input drift), in the
target labels (prediction drift), or in the underlying relationships between
the data and the target (concept drift). Detecting and adapting to data drift
is an essential part of maintaining long-term model performance.

\subsection{Drift Definitions}\label{subsec:drift_definitions}
Drift can manifest in several forms, each affecting the model in different
ways:

Input Drift: it refers to changes in the distribution of input features. For
example, new types of customer behavior or market conditions may introduce
patterns that the model has not seen before.

Prediction Drift: it occurs when the relationship between the input data and
the predicted labels changes over time. This might happen if the underlying
problem or task evolves, requiring the model to adapt its predictions.

Concept Drift: it refers to changes in the fundamental relationship between the
input data and the target variable. This type of drift is particularly
challenging as it can render a model's assumptions invalid, making it necessary
to retrain the model.

\subsection{Drift Detection}\label{subsec:drift_detection}
Drift detection aims to identify when a change in the data distribution has
occurred. Several techniques have been developed for detecting drift, including
statistical tests (e.g., Kolmogorov-Smirnov test), performance monitoring
(e.g., monitoring prediction accuracy), and data comparison methods (e.g.,
comparing distributions of recent and past data). The challenge in drift
detection lies in distinguishing between true drifts and natural fluctuations
in data, as well as determining when a drift is significant enough to warrant
model retraining.

\subsection{Drift Adaption}\label{subsec:drift_adaption}
Once drift has been detected, adaptation methods are employed to update the
model and maintain its performance. Adaptation strategies can include
retraining the model on recent data, adjusting model parameters, or
incorporating new data sources. In stream learning, the adaptation process must
be performed incrementally to handle the constant flow of new data.
Additionally, strategies for forgetting outdated information and retaining
relevant knowledge are critical for efficient drift adaptation, ensuring the
model remains responsive to evolving data.

\subsection{Drift Explainability}\label{subsec:drift_explainability}
Drift explainability focuses on understanding the nature and reasons behind
changes in data distributions that impact machine learning models. While
detecting drift is essential, it is equally important to comprehend why and how
the drift is occurring, as this provides insights into the model's evolving
behavior and helps guide model updates and maintenance.

In the context of machine learning, explaining drift involves identifying which
features have changed significantly and how these changes influence the model's
predictions. By using techniques such as feature importance analysis, it is
possible to trace the shifts in data that lead to altered model outcomes. This
helps practitioners understand the specific causes of drift, whether it stems
from changes in feature values, relationships between features, or the overall
data structure.

Drift explainability becomes crucial for making informed decisions about model
retraining and adaptation. Without understanding the underlying causes of
drift, it is difficult to assess whether model updates are necessary, and if
so, what specific adjustments should be made. This lack of insight could lead
to inefficient or unnecessary retraining, which may ultimately degrade model
performance.

In dynamic environments, where data continuously evolves, explaining drift also
allows for distinguishing between transient fluctuations and more persistent
changes. This understanding prevents premature retraining for short-term
variations, ensuring that updates occur only when the drift is substantial
enough to impact model accuracy.