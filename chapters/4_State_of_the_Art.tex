\chapter{State of the Art}\label{ch:state_of_the_art}

This chapter presents an overview of current developments in clustering methods
for evolving data streams, focusing on streaming, tracking, and dynamic
clustering approaches.

First, streaming clustering methods are examined, which incrementally process
data to manage high-velocity streams and adapt to changing data
characteristics. Next, cluster tracking techniques are reviewed, describing how
clusters can be matched, merged, split, or otherwise transformed over time as
data distributions evolve. Finally, dynamic clustering frameworks that combine
streaming and tracking functionalities are discussed, enabling real-time
cluster updates and interpretable analysis of cluster evolution.

This survey provides a foundation for understanding the challenges and
solutions in clustering evolving data, supporting the developments introduced
in subsequent chapters.

\section{Streaming Clustering Algorithms}\label{sec:streaming_clustering_algorithms}

Streaming clustering algorithms are designed to handle continuously arriving
data streams efficiently while adapting to changes in the underlying data
distribution. As discussed by Zubaroglu et
Atalay~\cite{streaming_clustering_review}, the main categories of streaming
clustering algorithms are: hierarchical, partitioning-based, density-based,
grid-based, and model-based methods.

\textbf{Hierarchical streaming clustering} algorithms produce nested clusters organized
in a tree-like structure called a dendrogram, which offers a rich and interpretable
summary of data relationships. These methods are divided into \emph{agglomerative} (bottom-up)
and \emph{divisive} (top-down) approaches. Agglomerative algorithms start by treating
each data point as a separate cluster and iteratively merge clusters based on similarity,
whereas divisive algorithms begin with all data points in a single cluster and recursively
split them. Although hierarchical methods provide detailed cluster structures, their high
computational complexity and sensitivity to noise and outliers make them less suitable for
high-velocity data streams. Examples include \textsc{BIRCH}~\cite{birch} and
\textsc{HUE-Stream}~\cite{hue_stream}.

\textbf{Partitioning-based streaming algorithms} divide data into a predefined number of clusters
by optimizing an objective function, commonly based on distance measures to cluster centroids. They
maintain compact data summaries such as micro-clusters or cluster feature vectors to efficiently update
clusters incrementally. These algorithms are computationally efficient and easy to implement; however,
they generally require prior knowledge of the number of clusters and tend to identify only spherical
or convex clusters. Notable examples include \textsc{CluStream}~\cite{clustream} and
\textsc{StreamKM++}~\cite{stream_km_plus_plus}.

\textbf{Density-based methods} identify clusters as regions of high data density separated by
regions of low density. In streaming contexts, they maintain summaries of dense micro-clusters
that evolve over time, allowing them to discover arbitrarily shaped clusters and automatically
detect the number of clusters. These methods also demonstrate robustness to noise and outliers.
However, they typically require multiple user-defined parameters and can struggle when clusters
have varying densities. Representative algorithms include \textsc{DenStream}~\cite{denstream}
and \textsc{D-Stream}~\cite{d_stream}.

\textbf{Grid-based streaming clustering} approaches partition the data space into a finite number
of cells or grids and cluster these cells based on their density or other summary statistics. This
discretization allows for fast updates and scalable clustering as runtime depends primarily on the
number of cells rather than the number of data points. Grid-based methods are naturally robust to
noise and can detect clusters of arbitrary shapes; however, their performance decreases in high-dimensional
spaces due to the exponential growth of grid cells. Additionally, the choice of grid granularity heavily
influences clustering results. Examples include \textsc{MR-Stream}~\cite{mr_stream}
and \textsc{CLIQUE}~\cite{clique}.

\textbf{Model-based streaming clustering algorithms} assume that data are generated from a mixture of
underlying probability distributions and aim to identify these models incrementally. These approaches
offer principled frameworks with strong noise robustness and can incorporate prior knowledge through
model selection. However, their effectiveness depends on the appropriateness of the selected model,
and they often involve complex parameter estimation procedures. Algorithms in this category include
\textsc{COBWEB}~\cite{cobweb} and \textsc{CluDistream}~\cite{cludistream}.

Each category offers distinct advantages and limitations, summarized in
Table~\ref{table:streaming-clustering-comparison}. Despite significant
progress, several challenges remain in streaming clustering research, such as
automatically adapting to concept drift, handling evolving cluster structures,
and providing explanations for cluster evolution. These gaps motivate the
development of dynamic clustering frameworks that combine streaming processing
with cluster tracking and evolution analysis, enabling advanced applications
including drift explainability.

\begin{table}[H]
    \centering

    \begin{tabular}{|l|p{6cm}|p{6cm}|}
        \hline
        \textbf{Category} & \textbf{Advantages}                                                 & \textbf{Disadvantages}                                                  \\
        \hline
        Hierarchical      & Provides detailed cluster hierarchy and interpretable dendrograms.  & High computational complexity; sensitive to noise and outliers.         \\
        Partitioning      & Efficient and easy to implement; suitable for large-scale data.     & Requires predefined number of clusters; limited to spherical clusters.  \\
        Density-Based     & Detects arbitrary shapes; robust to noise; adaptive cluster number. & Sensitive to parameter settings; struggles with multi-density clusters. \\
        Grid-Based        & Fast updates; scalable; noise robust; detects arbitrary shapes.     & Degrades in high dimensions; depends on grid size choice.               \\
        Model-Based       & Robust to noise; interpretable probabilistic framework.             & Depends heavily on model selection; computationally intensive.          \\
        \hline
    \end{tabular}
    \caption{Comparison of Streaming Clustering Approaches.}
    \label{table:streaming-clustering-comparison}
\end{table}

The following subsection introduces CluStream, a widely adopted framework for
dynamic clustering that serves as a foundational component in numerous existing
systems. Owing to its modular architecture and demonstrated practical
effectiveness, CluStream has been extensively implemented and integrated into
various real-time data stream clustering solutions.

\subsection{CluStream}\label{subsec:clustream}

CluStream~\cite{clustream} is a widely adopted framework designed for
clustering evolving data streams. It operates on a two-phase architecture that
separates online data summarization from offline cluster analysis, balancing
real-time efficiency with analytical depth.

\paragraph{Online Micro-Clustering} The online phase processes incoming data in real time, incrementally
summarizing the stream into a set of \emph{micro-clusters}. Each micro-cluster
is a temporal summary of a group of data points and is defined using the
Clustering Feature (CF) vector:

\begin{equation}
    \text{MC} = \left( \vec{LS}, SS, \vec{TS}, N \right)
\end{equation}

where:
\begin{itemize}
    \item $\vec{LS} = \sum_{i=1}^{N} \mathbf{x}_i$ is the linear sum of $N$ $d$-dimensional data points,
    \item $\vec{SS} = \sum_{i=1}^{N} \mathbf{x}_i \circ \mathbf{x}_i$ is the sum of squares,
    \item $\vec{TS} = \sum_{i=1}^{N} t_i$ is the sum of timestamps,
    \item $N$ is the number of points contained in the micro-cluster.
\end{itemize}

From these quantities, the following statistics can be derived:
\begin{itemize}
    \item \textbf{Centroid}:
          \begin{equation}
              \boldsymbol{\mu} = \frac{\vec{LS}}{N}
          \end{equation}
    \item \textbf{Standard Deviation}:
          \begin{equation}
              \sigma = \sqrt{ \frac{\vec{SS}}{N} - \left( \frac{\vec{LS}}{N} \right)^2 }
          \end{equation}
\end{itemize}

Incoming data points are assigned to the nearest micro-cluster if the distance
to its centroid is below a threshold, often proportional to the cluster's
standard deviation. Once a point $\mathbf{x}$ with timestamp $t$ is assigned to
a micro-cluster $MC_j$, its internal statistics are updated as follows:

\begin{equation}
    \label{eq:clustream_insert}
    \begin{cases}
        \vec{LS}_j & \leftarrow \vec{LS}_j + \mathbf{x}                  \\
        \vec{SS}_j & \leftarrow \vec{SS}_j + \mathbf{x} \circ \mathbf{x} \\
        \vec{TS}_j & \leftarrow \vec{TS}_j + t                           \\
        n_j        & \leftarrow n_j + 1
    \end{cases}
\end{equation}

If the point does not fit within any existing micro-cluster, a new one is
created using its statistics. Specifically, given a data point $ \mathbf{x} $
with timestamp $ t $, the new micro-cluster is initialized as:

\begin{equation}
    \label{eq:clustream_create}
    \begin{cases}
        \vec{LS} & = \mathbf{x}                  \\
        \vec{SS} & = \mathbf{x} \circ \mathbf{x} \\
        \vec{TS} & = t                           \\
        n        & = 1
    \end{cases}
\end{equation}

Since the micro-cluster contains only one point, no variance or standard
deviation can be computed yet. In practice, a default threshold is often used
for such singleton clusters.

To maintain a fixed number of micro-clusters (denoted by $q$), the algorithm
first attempts to remove any micro-cluster that has not been updated within the
time window. If no such outdated micro-cluster exists, it then merges the two
closest micro-clusters.

When two micro-clusters $MC_1$ and $MC_2$ are merged, the resulting
micro-cluster is defined as:
\begin{equation}
    \label{eq:clustream_merge}
    \begin{cases}
        \vec{LS}_{\text{new}} & = \vec{LS}_1 + \vec{LS}_2 \\
        \vec{SS}_{\text{new}} & = \vec{SS}_1 + \vec{SS}_2 \\
        \vec{TS}_{\text{new}} & = \vec{TS}_1 + \vec{TS}_2 \\
        N_{\text{new}}        & = N_1 + N_2
    \end{cases}
\end{equation}

\input{algorithms/clustream_online.tex}

\paragraph{Pyramidal Time Frame} To support temporal analysis, CluStream maintains snapshots of the current set
of micro-clusters at multiple temporal granularities. Snapshots are stored at
exponentially spaced intervals, forming a pyramidal time frame that allows
efficient reconstruction of historical cluster states with logarithmic storage
complexity.

\paragraph{Offline Macro-Clustering} At user-specified intervals or upon query, the offline phase performs
macro-clustering using the current or historical micro-cluster summaries. This
typically involves applying a conventional clustering algorithm (e.g., weighted
$k$-Means) to the centroids of the micro-clusters, where each centroid is
weighted by the number of data points $N$ it represents. This enables efficient
clustering over a condensed, noise-reduced representation of the data stream.

This modular architecture enables CluStream to efficiently handle high-speed
data streams while supporting accurate and scalable offline analysis.

\section{Tracking Clusters Algorithms}\label{sec:tracking_clusters_algorithms}

The challenge of tracking evolving clustering structures in non-stationary data
streams has gained increasing attention, leading to the development of numerous
methods designed to capture and explain transitions such as the emergence,
disappearance, splitting, merging, and persistence of clusters over time. This
section reviews the key methodologies corresponding to the transition types
outlined in Section~\ref{sec:prob_tracking_clusters}, organizing them according
to the algorithmic paradigms they follow, as also done by Atif et
al.~\cite{tracking_review}.

\textbf{Self-Organizing Maps (SOMs)} change tracking and it is build on the idea of
learning low-dimensional topological representations of high-dimensional temporal data.

Denny and Squire~\cite{som_tracking} proposed a method where \textsc{SOMs} are
trained sequentially on snapshots $D_{t_i}$ and $D_{t_{i+1}}$, with cluster
assignments determined using $k$-means. Although the method can track survival
and shifts in centroid position, it fails to reliably detect emerging or
disappearing clusters due to static initialization and dependency on prior
topology.

To address this, \textsc{ReDSOM}~\cite{redsom_tracking} was developed to
compare density distributions and centroid shifts between successive SOMs. It
supports identification of transitions such as:

\begin{itemize}
    \item Disappearance: Regions with reduced density and unmapped data.

    \item Appearance: Newly populated map regions.

    \item Enlargement/Contraction: Changes in the spread of mapped data.
\end{itemize}

However, SOM-based methods may struggle with overlapping or nested clusters due
to their grid-constrained topology.

\textbf{Heuristic and Graph-Based Algorithms} are distinct family of algorithms focuses
on tracking structural transitions through direct comparison of clustering results at
successive time points.

\textsc{MONIC} (Spiliopoulou et al.~\cite{monic}) is a foundational heuristic framework
for monitoring the evolution of clusters over time. Given two clustering solutions at
different time points, $C^t = \{C_1^t, C_2^t, \dots, C_{k_t}^t\}$ at time
$t$ and $C^{\tau} = \{C_1^{\tau}, C_2^{\tau}, \dots, C_{k_{\tau}}^{\tau}\}$ at a
later time $\tau > t$, the method constructs an \emph{overlap matrix} $O$ to quantify
the fraction of shared data points between clusters across time:

\begin{equation}
    O_{ij} = \frac{|C_i^t \cap C_j^{\tau}|}{|C_i^t|},
\end{equation}

where:
\begin{itemize}
    \item $C_i^t$ denotes the $i$-th cluster at time $t$,
    \item $C_j^{\tau}$ denotes the $j$-th cluster at time $\tau$,
    \item $k_t$ and $k_{\tau}$ are the number of clusters at times $t$ and $\tau$, respectively,
\end{itemize}

To decide whether clusters correspond across time, a threshold $ \varepsilon \in
    [0,1] $ is used: overlaps above $ \varepsilon $ are considered significant.

Using this threshold, cluster transitions are categorized as follows:

\begin{itemize}
    \item \textbf{Appearance}: Cluster $ C_j^{\tau} $ has no significant overlap with any cluster $ C_i^t $, i.e.,
          \[
              \nexists i: O_{ij} > \varepsilon.
          \]

    \item \textbf{Disappearance}: Cluster $ C_i^t $ has no significant overlap with any cluster $ C_j^{\tau} $, i.e.,
          \[
              \nexists j: O_{ij} > \varepsilon.
          \]

    \item \textbf{Merge/Split}: Multiple significant overlaps indicate that one cluster splits into several or several clusters merge, i.e., multiple $ O_{ij} > \varepsilon $ for fixed $ i $ or $ j $.

    \item \textbf{Survival}: A cluster persists over time with a one-to-one high overlap, i.e., there exists a unique $ (i,j) $ such that
          \[
              O_{ij} > \varepsilon,
          \]
          and this correspondence is exclusive.

    \item \textbf{Internal Transition}: The cluster survives but undergoes changes in its size or centroid position between $ t $ and $ \tau $.
\end{itemize}

This directly aligns with the transition framework from
Table~\ref{table:cluster_transitions} but requires pre-defined overlap
thresholds and can manage only clusters with enumeration representation.

\textsc{MONIC+} (Ntoutsi et al.~\cite{monic_plus}) generalizes the overlap function by incorporating cluster-type-specific
definitions. MONIC+ supports three distinct types of clusters based on how they are represented and formed:
\begin{itemize}
    \item Type A Clusters (Geometric Clusters): Discovered in a dataset-independent
          metric space, these clusters are defined by their geometric properties (e.g.,
          spheres in k-means). Cluster transitions are observed as geometric
          transformations over time.
          \begin{equation}
              \text{overlap}(C_i,C_j) = \frac{\text{area}(C_i \cap C_j)}{\text{area}(C_i)}
          \end{equation}
    \item Type B1 Clusters (Set-based Clusters): These clusters are defined extensionally
          as sets of data records without relying on a fixed metric space. Algorithms
          like hierarchical clustering fall under this category. The data-dependent
          nature of the metric space means that adding a new point can affect cluster
          boundaries indirectly.

          Here, the function $\text{age}(\mathbf{x}, t_j)$ measures the relevance of the
          data point $\mathbf{x}$ at time $t_j$, typically reflecting how recent or
          important the point is (e.g., an exponentially decaying weight based on
          timestamp). Using this, the overlap is defined as:
          \begin{equation}
              \text{overlap}(C_i,C_j) = \frac{\sum_{\mathbf{x} \in C_i \cap C_j} \text{age}(\mathbf{x}, t_j)}{\sum_{\mathbf{x} \in C_i} \text{age}(\mathbf{x}, t_j)}
          \end{equation}
    \item Type B2 Clusters (Distribution-based Clusters): Clusters here are defined
          intensionally as statistical distributions. A cluster $C_i$ is characterized by
          its cardinality $|C_i|$, mean $\boldsymbol{\mu}(C_i)$, and standard deviation $\sigma(C_i)$.
          The Expectation-Maximization (EM) algorithm is a typical example.
          \begin{equation}
              \text{overlap}(C_i, C_j) =
              \begin{cases}
                  1 - \dfrac{||\boldsymbol{\mu}(C_i) - \boldsymbol{\mu}(C_j)||_2}{\sigma(c_i)} & \text{if } ||\boldsymbol{\mu}(C_i) - \boldsymbol{\mu}(C_j)||_2 \leq \sigma(C_i) \\
                  0                                              & \text{otherwise}
              \end{cases}
          \end{equation}
\end{itemize}

\textsc{MClusT} (Oliveira and Gama~\cite{mclust}) extends MONIC by representing transitions via a
bipartite graph weighted by conditional probabilities between clusters over time.
This probabilistic approach refines the detection of transitions and facilitates intuitive
visualization of merges and splits.

Let $ \mathbf{z} $ denote a data point such that $ \mathbf{z} \in C_i^t $. The
weight of an edge from cluster $ C_i^t $ to cluster $ C_j^{\tau} $ is given by
the conditional probability that the same point $ \mathbf{z} $ belongs to $
    C_i^t $, given it belonged to $ C_j^{\tau} $:
\begin{equation}
    \text{weight}(C_i^t, C_j^{\tau}) = P(\mathbf{z} \in C_j^{\tau} \mid \mathbf{z} \in C_i^t) =
    \frac{P(\mathbf{z} \in C_i^t \cap C_j^{\tau})}{P(\mathbf{z} \in C_i^t)}.
\end{equation}

\textsc{MEC} (Oliveira and Gama~\cite{mec}) introduces an alternative bipartite-graph-based visualization
tool, emphasizing structural comparison of clusters at static intervals. It uses conditional
probability edges to identify transitions similar to MONIC, reinforcing the notion of transition
tracking via inter-temporal matching.

\textbf{Evolutionary Clustering}'s fundamental concept is to balance temporal smoothness
with clustering accuracy at each time point. Chakrabarti et al.~\cite{evolutionary_clustering}
introduced the evolutionary clustering framework, in which the objective is to compute clustering
solutions $ C^t $ for each time step $ t $ that are both representative of the current data snapshot
and consistent with past clusterings. Let $ D^t $ denote the dataset observed at time $ t $.
The trade-off is formalized as:
\begin{equation}
    \sum_{t=1}^T sq(C^t, D^t) - cp \sum_{t=2}^T hc(C^{t-1}, C^t)
\end{equation}

Here, $ sq $ represents snapshot quality measuring how well clustering $ C^t $
fits the data $ D^t $, $ hc $ denotes the history cost measuring changes
between consecutive clusterings, and $ cp $ is a change penalty. Transitions
such as \emph{survival} and \emph{small structural shifts} are naturally
captured through low history cost values, while abrupt transitions (e.g.,
\emph{split}, \emph{merge}, or \emph{appearance}) incur higher costs unless
strongly justified by the snapshot quality.

Chi et al.~\cite{spectral_evolutionary_clustering} extended traditional
clustering methods through evolutionary spectral clustering, which optimizes a
combined cost function $J_{\text{total}}$ defined as:
\begin{equation}
    J_{\text{total}} = \alpha J_{\text{temporal}} + (1 - \alpha) J_{\text{snapshot}},
\end{equation}
where $0 \leq \alpha \leq 1$. Here, $J_{\text{snapshot}}$ measures the quality
of the clustering at the current time step (i.e., how well the data is partitioned),
while $J_{\text{temporal}}$ ensures temporal smoothness by evaluating how
consistent the current clustering is with historical clustering solutions.

Zhang et al.~\cite{density_evolutionary_clustering} adapted the framework for
density-based clustering (evolutionary DBSCAN) using a similar cost function.
This variant directly supports transitions like appearance and disappearance of
clusters by relying on density thresholds, but it requires parameter tuning
that limits scalability.

Xu et al.~\cite{adaptive_evolutionary_clustering} introduced an adaptive
evolutionary framework that updates proximity matrices recursively. This
supports continuous tracking of survival, gradual merges, or splits, but the
approach is computationally intensive and NP-hard in general.

\section{Dynamic Clustering Algorithms}\label{sec:dynamic_clustering_algorithms}

In recent years, several works have been proposed to address the dynamic
clustering problem by integrating streaming clustering and cluster evolution
tracking into unified frameworks. These approaches aim not only to detect
meaningful cluster structures in evolving data streams but also to monitor how
these clusters evolve over time, capturing the transitions. The primary
challenges lie in efficiently processing high-velocity data, detecting
structural changes in the clustering, and distinguishing between transient and
persistent transitions.

Namitha et al.~\cite{namitha_dynamic_clustering_1} proposed a method that
combines CluStream with an estimation of the number of macro-clusters using the
Ordered Multiple Runs of k-Means (OMRk)~\cite{omrk} algorithm. A Simplified
Silhouette index is used as the relative clustering validity criterion.
Macro-clustering is performed over fixed-size windows. Their model explicitly
considers external transitions such as \emph{survival}, \emph{absorption}, and
\emph{split}:

Let $ C^t $ and $ C^\tau $ denote the sets of macro-clusters obtained from two
consecutive fixed-size time windows $ t $ and $ \tau $, respectively.

\begin{itemize}
    \item \textbf{Survival:} A cluster $ C_i^{t} \in C^t $ is said to have
          survived as $ C_j^{\tau} \in C^\tau $ if:
          \[
              \begin{cases}
                  ||\boldsymbol{\mu}(C_i^{t}) - \boldsymbol{\mu}(C_j^{\tau})||_2 \le r(C_i^{t}) \\
                  \frac{r(C_j^{\tau})}{r(C_i^t)} \in [0.75, 1.25]
              \end{cases}
          \]

    \item \textbf{Absorption:} A cluster $ C_i^{t} \in C^t $ is said to be
          absorbed by $ C_j^{\tau} \in C^\tau $ if:
          \[
              \begin{cases}
                  ||\boldsymbol{\mu}(C_i^{t}) - \boldsymbol{\mu}(C_j^{\tau})||_2 \le r(C_j^{\tau}) \\
                  r(C_i^{t}) \le ||\boldsymbol{\mu}(C_i^{t}) - \boldsymbol{\mu}(C_j^{\tau})||_2
              \end{cases}
          \]

    \item \textbf{Split:} A cluster $ C_i^{t} \in C^t $ is said to have split into a set of clusters
          $ \{C_{j_1}^{\tau}, C_{j_2}^{\tau}, \dots, C_{j_m}^{\tau}\} \subseteq C^\tau $ if:
          \[
              \begin{cases}
                  ||\boldsymbol{\mu}(C_i^{t}) - \boldsymbol{\mu}(C_{j_k}^{\tau})||_2 \le r(C_i^{t}) & \forall k \in \{1, \dots, m\} \\
                  r(C_{j_k}^{\tau}) < r(C_i^{t})                        & \forall k \in \{1, \dots, m\} \\
                  \frac{1}{|C_i^{t}|} \sum_{k=1}^{m} |C_{j_k}^{\tau}| \in [0.6, 1]
              \end{cases}
          \]
\end{itemize}

Clusters that do not satisfy any of the above criteria are classified as
\textbf{disappeared} if they belong to $ C^t $ but have no corresponding match
in $ C^\tau $, or as \textbf{appeared} if they appear in $ C^\tau $ without
being derived from any cluster in $ C^t $.

In a subsequent work, Namitha et al.~\cite{namitha_dynamic_clustering_2}
proposed replacing the fixed-size window approach with a drift detection
mechanism based on the \emph{Page-Hinkley test}~\cite{page_hinkley}. For each new data
point, the distance to the nearest cluster is monitored to detect
distributional changes. This approach triggers macro-clustering only when
necessary and explicitly integrates the MONIC algorithm for cluster transition
tracking.

Lima and Sousa introduced CETra (Cluster Evolution Tracker)~\cite{cetra}, an
online cluster tracking framework tailored for disjoint clusters that evolve
gradually. CETra maintains a \emph{referential clustering} $C^{\text{ref}}$
in memory, used as a basis for comparison with successive \emph{evolutionary
    clusterings} $C^{\tau}$:

\begin{itemize}
    \item A clustering remains referential until a significant divergence from subsequent
          clusterings is observed.
    \item This approach allows CETra to distinguish between gradual evolution and
          transient changes.
    \item Both external and internal transitions are considered, enabling a complete
          evolution tracking mechanism.
\end{itemize}

Feng et al.\ proposed OCEAN~\cite{ocean}, a completely online clustering
algorithm based on a grid-partitioning scheme. OCEAN reduces spatial complexity
by:

\begin{itemize}
    \item Maintaining concrete cluster statistics in each grid cell for real-time
          processing.
    \item Employing an adaptive composite windowing strategy that combines current and
          historical data windows, dynamically adjusting their widths.
    \item Supporting multiple clustering scenarios and providing evolution analysis
          through the composite window to improve clustering efficiency.
\end{itemize}

Overall, dynamic clustering methods aim to efficiently and accurately detect
cluster evolution over time. While some rely on summarization techniques and
periodic re-clustering, others emphasize online monitoring and drift detection
to adapt to changes in real time.