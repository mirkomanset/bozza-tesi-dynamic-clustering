\chapter{State of the Art}\label{ch:state_of_the_art}

\section{Streaming Clustering Algorithms}\label{sec:streaming_clustering_algorithms}

Streaming clustering algorithms are designed to handle continuously arriving
data streams efficiently while adapting to changes in the underlying data
distribution. Unlike traditional batch clustering methods, streaming clustering
must operate under constraints such as limited memory, the impossibility of
revisiting old data, and the need for real-time processing. As discussed by
Zubaroglu et Atalay~\cite{streaming_clustering_review}, the main categories of
streaming clustering algorithms are: hierarchical, partitioning-based,
density-based, grid-based, and model-based methods.

\textbf{Hierarchical streaming clustering} algorithms produce nested clusters organized
in a tree-like structure called a dendrogram, which offers a rich and interpretable
summary of data relationships. These methods are divided into \textit{agglomerative} (bottom-up)
and \textit{divisive} (top-down) approaches. Agglomerative algorithms start by treating
each data point as a separate cluster and iteratively merge clusters based on similarity,
whereas divisive algorithms begin with all data points in a single cluster and recursively
split them. Although hierarchical methods provide detailed cluster structures, their high
computational complexity and sensitivity to noise and outliers make them less suitable for
high-velocity data streams. Examples include \textsc{BIRCH}~\cite{birch} and
\textsc{HUE-Stream}~\cite{hue_stream}.

\textbf{Partitioning-based streaming algorithms} divide data into a predefined number of clusters
by optimizing an objective function, commonly based on distance measures to cluster centroids. They
maintain compact data summaries such as micro-clusters or cluster feature vectors to efficiently update
clusters incrementally. These algorithms are computationally efficient and easy to implement; however,
they generally require prior knowledge of the number of clusters and tend to identify only spherical
or convex clusters. Notable examples include \textsc{CluStream}~\cite{clustream} and
\textsc{StreamKM++}~\cite{stream_km_plus_plus}.

\textbf{Density-based methods} identify clusters as regions of high data density separated by
regions of low density. In streaming contexts, they maintain summaries of dense micro-clusters
that evolve over time, allowing them to discover arbitrarily shaped clusters and automatically
detect the number of clusters. These methods also demonstrate robustness to noise and outliers.
However, they typically require multiple user-defined parameters and can struggle when clusters
have varying densities. Representative algorithms include \textsc{DenStream}~\cite{denstream}
and \textsc{D-Stream}~\cite{d_stream}.

\textbf{Grid-based streaming clustering} approaches partition the data space into a finite number
of cells or grids and cluster these cells based on their density or other summary statistics. This
discretization allows for fast updates and scalable clustering as runtime depends primarily on the
number of cells rather than the number of data points. Grid-based methods are naturally robust to
noise and can detect clusters of arbitrary shapes; however, their performance decreases in high-dimensional
spaces due to the exponential growth of grid cells. Additionally, the choice of grid granularity heavily
influences clustering results. Examples include \textsc{MR-Stream}~\cite{mr_stream}
and \textsc{CLIQUE}~\cite{clique}.

\textbf{Model-based streaming clustering algorithms} assume that data are generated from a mixture of
underlying probability distributions and aim to identify these models incrementally. These approaches
offer principled frameworks with strong noise robustness and can incorporate prior knowledge through
model selection. However, their effectiveness depends on the appropriateness of the selected model,
and they often involve complex parameter estimation procedures. Algorithms in this category include
\textsc{COBWEB}~\cite{cobweb} and \textsc{CluDistream}~\cite{cludistream}.

\begin{table}[H]
    \centering

    \begin{tabular}{|l|p{6cm}|p{6cm}|}
        \hline
        \textbf{Category} & \textbf{Advantages}                                                & \textbf{Disadvantages}                                                 \\
        \hline
        Hierarchical      & Provides detailed cluster hierarchy and interpretable dendrograms  & High computational complexity; sensitive to noise and outliers         \\
        Partitioning      & Efficient and easy to implement; suitable for large-scale data     & Requires predefined number of clusters; limited to spherical clusters  \\
        Density-Based     & Detects arbitrary shapes; robust to noise; adaptive cluster number & Sensitive to parameter settings; struggles with multi-density clusters \\
        Grid-Based        & Fast updates; scalable; noise robust; detects arbitrary shapes     & Degrades in high dimensions; depends on grid size choice               \\
        Model-Based       & Robust to noise; interpretable probabilistic framework             & Depends heavily on model selection; computationally intensive          \\
        \hline
    \end{tabular}
    \caption{Comparison of Streaming Clustering Approaches}
    \label{table:streaming-clustering-comparison}
\end{table}

Each category offers distinct advantages and limitations, summarized in Table
~\ref{table:streaming-clustering-comparison}. Despite significant progress,
several challenges remain in streaming clustering research, such as
automatically adapting to concept drift, handling evolving cluster structures,
and providing explanations for cluster evolution. These gaps motivate the
development of dynamic clustering frameworks that combine streaming processing
with cluster tracking and evolution analysis, enabling advanced applications
including drift explainability.

The following subsection introduces CluStream, a widely adopted framework for
dynamic clustering that serves as a foundational component in numerous existing
systems. Owing to its modular architecture and demonstrated practical
effectiveness, CluStream has been extensively implemented and integrated into
various real-time data stream clustering solutions.

\subsection{CluStream}\label{subsec:clustream}

CluStream~\cite{clustream} is a widely adopted framework designed for
clustering evolving data streams. It operates on a two-phase architecture that
separates online data summarization from offline cluster analysis, balancing
real-time efficiency with analytical depth.

\paragraph{Online Micro-Clustering} The online phase processes incoming data in real time, incrementally
summarizing the stream into a set of \textit{micro-clusters}. Each
micro-cluster is a temporal summary of a group of data points and is defined
using a variant of the Clustering Feature (CF) vector:

\begin{equation}
    \text{MC} = \left( \vec{LS}, SS, \vec{TS}, n \right)
\end{equation}

where:
\begin{itemize}
    \item $\vec{LS} = \sum_{i=1}^{n} \vec{x}_i$ is the linear sum of $n$ $d$-dimensional data points,
    \item $\vec{SS} = \sum_{i=1}^{n} \vec{x}_i \cdot \vec{x}_i$ is the sum of squares,
    \item $\vec{TS} = \sum_{i=1}^{n} t_i$ is the sum of timestamps,
    \item $n$ is the number of points contained in the micro-cluster.
\end{itemize}

From these quantities, the following statistics can be derived:
\begin{itemize}
    \item \textbf{Centroid}:
          \begin{equation}
              \vec{\mu} = \frac{\vec{LS}}{n}
          \end{equation}
    \item \textbf{Standard Deviation}:
          \begin{equation}
              \sigma = \sqrt{ \frac{\vec{SS}}{n} - \left( \frac{\vec{LS}}{n} \right)^2 }
          \end{equation}
\end{itemize}

Incoming data points are assigned to the nearest micro-cluster if the distance
to its centroid is below a threshold, often proportional to the cluster's
standard deviation. If the point does not fit within any existing
micro-cluster, a new one is created. To maintain a fixed budget of
micro-clusters (denoted by $q$), the algorithm either merges the two closest
micro-clusters or removes the least recently updated one.

When two micro-clusters $MC_1$ and $MC_2$ are merged, the resulting
micro-cluster is defined as:

\begin{align}
    \vec{LS}^{\text{new}} & = \vec{LS}^{(1)} + \vec{LS}^{(2)} \\
    \vec{SS}^{\text{new}} & = \vec{SS}^{(1)} + \vec{SS}^{(2)} \\
    \vec{TS}^{\text{new}} & = \vec{TS}^{(1)} + \vec{TS}^{(2)} \\
    n^{\text{new}}        & = n^{(1)} + n^{(2)}
\end{align}

\paragraph{Pyramidal Time Frame} To support temporal analysis, CluStream maintains snapshots of the current set
of micro-clusters at multiple temporal granularities. Snapshots are stored at
exponentially spaced intervals (e.g., 1, 2, 4, 8, $\dots$ time units), forming
a pyramidal time frame that allows efficient reconstruction of historical
cluster states with logarithmic storage complexity.

\paragraph{Offline Macro-Clustering} At user-specified intervals or upon query, the offline phase performs
macro-clustering using the current or historical micro-cluster summaries. This
typically involves applying a conventional clustering algorithm (e.g., weighted
$k$-Means) to the centroids of the micro-clusters, where each centroid is
weighted by the number of data points $n$ it represents. This enables efficient
clustering over a condensed, noise-reduced representation of the data stream.

This modular architecture enables CluStream to efficiently handle high-speed
data streams while supporting accurate and scalable offline analysis.

\section{Tracking Clusters Algorithms}\label{sec:tracking_clusters_algorithms}

The challenge of tracking evolving clustering structures in non-stationary data
streams has garnered increasing attention, leading to the development of
numerous methods designed to capture and explain transitions such as the
emergence, disappearance, splitting, merging, and persistence of clusters over
time. This section reviews the key methodologies corresponding to the
transition types outlined in Section~\ref{sec:prob_tracking_clusters},
organizing them according to the algorithmic paradigms they follow, as also
done by Atif et al.~\cite{tracking_review}.

\textbf{Self-Organizing Maps (SOMs)} change tracking and it is build on the idea of
learning low-dimensional topological representations of high-dimensional temporal data.

Denny and Squire~\cite{som_tracking} proposed a method where \textsc{SOMs} are
trained sequentially on snapshots $D_{t_i}$ and $D_{t_{i+1}}$, with cluster
assignments determined using $k$-means. Although the method can track survival
and shifts in centroid position, it fails to reliably detect emerging or
disappearing clusters due to static initialization and dependency on prior
topology.

To address this, \textsc{ReDSOM}~\cite{redsom_tracking} was developed to
compare density distributions and centroid shifts between successive SOMs. It
supports identification of transitions such as:

\begin{itemize}
    \item Disappearance: Regions with reduced density and unmapped data.

    \item Appearance: Newly populated map regions.

    \item Enlargement/Contraction: Changes in the spread of mapped data.
\end{itemize}

However, SOM-based methods may struggle with overlapping or nested clusters due
to their grid-constrained topology.

\textbf{Heuristic and Graph-Based Algorithms} are distinct family of algorithms focuses
on tracking structural transitions through direct comparison of clustering results at
successive time points.

\textsc{MONIC} (Spiliopoulou et al.~\cite{monic}) is a seminal heuristic framework that uses an overlap
matrix $O_{ij}$ between clusters $X_i$ at $t$ and $Y_j$ at $\tau > t$.
The overlap matrix is defined as:
\begin{equation}
    O_{ij} = \frac{|X_i \cap Y_j|}{|X_i|}, \quad i = 1, \ldots, k_1, \quad j = 1, \ldots, k_2.
\end{equation}

Transitions are categorized as:
\begin{itemize}
    \item Appearance: $\nexists X_i: O_{ij} > \theta$

    \item Disappearance: $\nexists Y_j: O_{ij} > \theta$

    \item Merge/Split: Multiple significant overlaps across time

    \item Survival: One-to-one high overlap

    \item Internal Transition: Changes in size or centroid within a surviving cluster
\end{itemize}

This directly aligns with the transition framework from
Table~\ref{table:cluster_transitions} but requires pre-defined overlap
thresholds and can manage only clusters with enumeration representation.

\textsc{MONIC+} (Ntoutsi et al.~\cite{monic_plus}) generalizes the overlap function by incorporating cluster-type-specific
definitions. MONIC+ supports three distinct types of clusters based on how they are represented and formed:
\begin{itemize}
    \item Type A Clusters (Geometric Clusters): Discovered in a dataset-independent
          metric space, these clusters are defined by their geometric properties (e.g.,
          spheres in k-means). Cluster transitions are observed as geometric
          transformations over time.
          \begin{equation}
              \text{overlap}(X,Y) = \frac{\text{area}(X \cap Y)}{\text{area}(X)}
          \end{equation}
    \item Type B1 Clusters (Set-based Clusters): These clusters are defined extensionally
          as sets of data records without relying on a fixed metric space. Algorithms
          like hierarchical clustering fall under this category. The data-dependent
          nature of the metric space means that adding a new point can affect cluster
          boundaries indirectly.
          \begin{equation}
              \text{overlap}(X,Y) = \frac{\sum_{a \in X \cap Y} \text{age}(a, t_j)}{\sum_{a \in X} \text{age}(a, t_j)}
          \end{equation}
    \item Type B2 Clusters (Distribution-based Clusters): Clusters here are defined
          intensionally as statistical distributions. A cluster $X$ is characterized by
          its cardinality $\text{card}(X)$, mean $\mu(X)$, and standard deviation
          $\sigma(X)$. The Expectation-Maximization (EM) algorithm is a typical example.
          \begin{equation}
              \text{overlap}(X, Y) =
              \begin{cases}
                  1 - \dfrac{|\mu(X) - \mu(Y)|}{\sigma(X)} & \text{if } |\mu(X) - \mu(Y)| \leq \sigma(X) \\
                  0                                        & \text{otherwise}
              \end{cases}
          \end{equation}
\end{itemize}

\textsc{MClusT} (Oliveira and Gama~\cite{mclust}) extends MONIC by representing transitions via a
bipartite graph weighted by conditional probabilities between clusters over time.
This probabilistic approach refines the detection of transitions and facilitates intuitive
visualization of merges and splits.
\begin{equation}
    \text{weight}(C_u, C_m) = P(X \in C_u(t_j) \mid X \in C_m(t_i)) =
    \frac{\sum_{m=1}^p P(X \in C_m(t_i) \cap C_u(t_j))}{\sum_{m=1}^p P(X \in C_m(t_i))}
\end{equation}

\textsc{MEC} (Oliveira and Gama~\cite{mec}) introduces an alternative bipartite-graph-based visualization
tool, emphasizing structural comparison of clusters at static intervals. It uses conditional
probability edges to identify transitions similar to MONIC, reinforcing the notion of transition
tracking via inter-temporal matching.

\textbf{Evolutionary Clustering} foundamental concept is to balance temporal smoothness
with clustering accuracy at each time point. Chakrabarti et al.~\cite{evolutionary_clustering}
introduced the evolutionary clustering framework, in which the objective is to compute clustering
solutions $C_t$ for each time step $t$ that are both representative of the current data snapshot
and consistent with past clusterings. The trade-off is formalized as:
\begin{equation}
    \sum_{t=1}^T sq(C_t, M_t) - cp \sum_{t=2}^T hc(C_{t-1}, C_t)
\end{equation}

Here, $sq$ represents snapshot quality, $hc$ denotes history cost, and $cp$ is
a change penalty. Transitions such as \textit{survival} and \textit{small
    structural shifts} are naturally captured through low history cost values,
while abrupt transitions (e.g., \textit{split}, \textit{merge}, or
\textit{appearance}) incur higher costs unless strongly justified by the
snapshot quality.

Chi et al.~\cite{spectral_evolutionary_clustering} extended this approach via
evolutionary spectral clustering, optimizing a cost of the form:
\begin{equation}
    C_{total} = \alpha C_{temporal} + (1-\alpha)C_{snapshot}
\end{equation}

Zhang et al.~\cite{density_evolutionary_clustering} adapted the framework for
density-based clustering (evolutionary DBSCAN) using a similar cost function.
This variant directly supports transitions like appearance and disappearance of
clusters by relying on density thresholds, but it requires parameter tuning
that limits scalability.

Xu et al.~\cite{adaptive_evolutionary_clustering} introduced an adaptive
evolutionary framework that updates proximity matrices recursively. This
supports continuous tracking of survival, gradual merges, or splits, but the
approach is computationally intensive and NP-hard in general.

\section{Dynamic Clustering Algorithms}\label{sec:dynamic_clustering_algorithms}

In recent years, several works have been proposed to address the dynamic
clustering problem by integrating streaming clustering and cluster evolution
tracking into unified frameworks. These approaches aim not only to detect
meaningful cluster structures in evolving data streams but also to monitor how
these clusters evolve over timeâ€”capturing events such as survival, absorption,
split, disappearance, and emergence. The primary challenges lie in efficiently
processing high-velocity data, detecting structural changes in the clustering,
and distinguishing between transient and persistent transitions.

Namitha et al.~\cite{namitha_dynamic_clustering_1} proposed a method that
combines CluStream with an estimation of the number of macro-clusters using the
Ordered Multiple Runs of k-Means (OMRk)~\cite{omrk} algorithm. A Simplified
Silhouette index is used as the relative clustering validity criterion.
Macro-clustering is performed over fixed-size windows. Their model explicitly
considers external transitions such as \textit{survival}, \textit{absorption},
and \textit{split}:

\begin{itemize}
    \item \textbf{Survival:} A cluster $X \in C_i$ is said to have survived as $Y \in C_j$ if:
          \begin{enumerate}
              \item The distance between the centroids of $X$ and $Y$ is less than the radius of
                    $X$.
              \item The ratio of the radii of $Y$ and $X$ lies between 0.75 and 1.25.
          \end{enumerate}

    \item \textbf{Absorption:} A cluster $X \in C_i$ is said to be absorbed by $Y \in C_j$ if:
          \begin{enumerate}
              \item The distance between the centroids of $X$ and $Y$ is less than the radius of
                    $Y$.
              \item The radius of $X$ is less than the distance between their centroids.
          \end{enumerate}

    \item \textbf{Split:} A cluster $X \in C_i$ is said to split into clusters $Y_1, Y_2, \dots$ in $C_j$ if:
          \begin{enumerate}
              \item The centroids of $Y_1, Y_2, \dots$ fall within the radius of $X$.
              \item Their radii are smaller than the radius of $X$.
              \item The ratio of the total number of points in $Y_1, Y_2, \dots$ to the number of
                    points in $X$ is between 0.6 and 1.
          \end{enumerate}
\end{itemize}

Clusters that do not fall into any of these categories are classified as
\textbf{disappeared} (in $C_j$) or \textbf{emerged} (in $C_j$ but not derived
from $C_i$).

In a subsequent work, Namitha et al.~\cite{namitha_dynamic_clustering_2}
proposed replacing the fixed-size window approach with a drift detection
mechanism based on the Page-Hinkley test~\cite{page_hinkley}. For each new data
point, the distance to the nearest cluster is monitored to detect
distributional changes. This approach triggers macro-clustering only when
necessary and explicitly integrates the MONIC algorithm for cluster transition
tracking.

Lima and Sousa introduced CETra (Cluster Evolution Tracker)~\cite{cetra}, an
online cluster tracking framework tailored for disjoint clusters that evolve
gradually. CETra maintains a \textit{referential clustering}
$\zeta_{\text{ref}}^{t_i}$ in memory, used as a basis for comparison with
successive \textit{evolutionary clusterings} $\zeta^{t_j}$:

\begin{itemize}
    \item A clustering remains referential until a significant divergence from subsequent
          clusterings is observed.
    \item This approach allows CETra to distinguish between gradual evolution and
          transient changes.
    \item Both external and internal transitions are considered, enabling a complete
          evolution tracking mechanism.
\end{itemize}

Feng et al.\ proposed OCEAN~\cite{ocean}, a completely online clustering
algorithm based on a grid-partitioning scheme. OCEAN reduces spatial complexity
by:

\begin{itemize}
    \item Maintaining concrete cluster statistics in each grid cell for real-time
          processing.
    \item Employing an adaptive composite windowing strategy that combines current and
          historical data windows, dynamically adjusting their widths.
    \item Supporting multiple clustering scenarios and providing evolution analysis
          through the composite window to improve clustering efficiency.
\end{itemize}

Overall, dynamic clustering methods aim to efficiently and accurately detect
cluster evolution over time. While some rely on summarization techniques and
periodic re-clustering, others emphasize online monitoring and drift detection
to adapt to changes in real time.